"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from cribl import utils
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from typing import Any, Optional

class OutputGooglePubsubAuthenticationMethod(str, Enum):
    r"""Google authentication method. Choose Auto to use environment variables PUBSUB_PROJECT and PUBSUB_CREDENTIALS.."""
    SECRET = 'secret'
    MANUAL = 'manual'

class OutputGooglePubsubBackpressureBehavior(str, Enum):
    r"""Whether to block, drop, or queue events when all receivers are exerting backpressure."""
    QUEUE = 'queue'
    DROP = 'drop'
    BLOCK = 'block'

class OutputGooglePubsubCompression(str, Enum):
    r"""Codec to use to compress the persisted data."""
    NONE = 'none'
    GZIP = 'gzip'



@dataclasses.dataclass
class OutputGooglePubsubPqControls:
    pass

class OutputGooglePubsubQueueFullBehavior(str, Enum):
    r"""Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    BLOCK = 'block'
    DROP = 'drop'

class OutputGooglePubsubRegion(str, Enum):
    r"""Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    PUBSUB_GOOGLEAPIS_COM = 'pubsub.googleapis.com'
    US_EAST1_PUBSUB_GOOGLEAPIS_COM = 'us-east1-pubsub.googleapis.com'
    US_EAST4_PUBSUB_GOOGLEAPIS_COM = 'us-east4-pubsub.googleapis.com'
    US_CENTRAL1_PUBSUB_GOOGLEAPIS_COM = 'us-central1-pubsub.googleapis.com'
    US_CENTRAL2_PUBSUB_GOOGLEAPIS_COM = 'us-central2-pubsub.googleapis.com'
    US_WEST1_PUBSUB_GOOGLEAPIS_COM = 'us-west1-pubsub.googleapis.com'
    US_WEST2_PUBSUB_GOOGLEAPIS_COM = 'us-west2-pubsub.googleapis.com'
    US_WEST3_PUBSUB_GOOGLEAPIS_COM = 'us-west3-pubsub.googleapis.com'
    SOUTHAMERICA_EAST1_PUBSUB_GOOGLEAPIS_COM = 'southamerica-east1-pubsub.googleapis.com'
    NORTHAMERICA_NORTHEAST1_PUBSUB_GOOGLEAPIS_COM = 'northamerica-northeast1-pubsub.googleapis.com'
    EUROPE_WEST6_PUBSUB_GOOGLEAPIS_COM = 'europe-west6-pubsub.googleapis.com'
    EUROPE_WEST4_PUBSUB_GOOGLEAPIS_COM = 'europe-west4-pubsub.googleapis.com'
    EUROPE_WEST3_PUBSUB_GOOGLEAPIS_COM = 'europe-west3-pubsub.googleapis.com'
    EUROPE_WEST2_PUBSUB_GOOGLEAPIS_COM = 'europe-west2-pubsub.googleapis.com'
    EUROPE_WEST1_PUBSUB_GOOGLEAPIS_COM = 'europe-west1-pubsub.googleapis.com'
    EUROPE_NORTH1_PUBSUB_GOOGLEAPIS_COM = 'europe-north1-pubsub.googleapis.com'
    AUSTRALIA_SOUTHEAST1_PUBSUB_GOOGLEAPIS_COM = 'australia-southeast1-pubsub.googleapis.com'
    ASIA_SOUTHEAST1_PUBSUB_GOOGLEAPIS_COM = 'asia-southeast1-pubsub.googleapis.com'
    ASIA_SOUTH1_PUBSUB_GOOGLEAPIS_COM = 'asia-south1-pubsub.googleapis.com'
    ASIA_NORTHEAST3_PUBSUB_GOOGLEAPIS_COM = 'asia-northeast3-pubsub.googleapis.com'
    ASIA_NORTHEAST2_PUBSUB_GOOGLEAPIS_COM = 'asia-northeast2-pubsub.googleapis.com'
    ASIA_NORTHEAST1_PUBSUB_GOOGLEAPIS_COM = 'asia-northeast1-pubsub.googleapis.com'
    ASIA_EAST2_PUBSUB_GOOGLEAPIS_COM = 'asia-east2-pubsub.googleapis.com'
    ASIA_EAST1_PUBSUB_GOOGLEAPIS_COM = 'asia-east1-pubsub.googleapis.com'

class OutputGooglePubsubType(str, Enum):
    GOOGLE_PUBSUB = 'google_pubsub'


@dataclass_json(undefined=Undefined.EXCLUDE)

@dataclasses.dataclass
class OutputGooglePubsub:
    topic_name: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('topicName') }})
    r"""ID of the topic to send events to."""
    type: OutputGooglePubsubType = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type') }})
    batch_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('batchSize'), 'exclude': lambda f: f is None }})
    r"""The maximum number of items the Google API should batch before it sends them to the topic."""
    batch_timeout: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('batchTimeout'), 'exclude': lambda f: f is None }})
    r"""The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached)."""
    create_topic: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('createTopic'), 'exclude': lambda f: f is None }})
    r"""If enabled, create topic if it does not exist."""
    environment: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('environment'), 'exclude': lambda f: f is None }})
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    flush_period_sec: Optional[Any] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('flushPeriodSec'), 'exclude': lambda f: f is None }})
    r"""Maximum time to wait before sending a batch (when Max batch size is not reached)."""
    google_auth_method: Optional[OutputGooglePubsubAuthenticationMethod] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('googleAuthMethod'), 'exclude': lambda f: f is None }})
    r"""Google authentication method. Choose Auto to use environment variables PUBSUB_PROJECT and PUBSUB_CREDENTIALS.."""
    id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id'), 'exclude': lambda f: f is None }})
    r"""Unique ID for this output"""
    max_in_progress: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxInProgress'), 'exclude': lambda f: f is None }})
    r"""The maximum number of in-progress API requests before backpressure is applied."""
    max_queue_size: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxQueueSize'), 'exclude': lambda f: f is None }})
    r"""Maximum number of queued batches before blocking."""
    max_record_size_kb: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxRecordSizeKB'), 'exclude': lambda f: f is None }})
    r"""Maximum size (KB) of batches to send."""
    on_backpressure: Optional[OutputGooglePubsubBackpressureBehavior] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('onBackpressure'), 'exclude': lambda f: f is None }})
    r"""Whether to block, drop, or queue events when all receivers are exerting backpressure."""
    ordered_delivery: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('orderedDelivery'), 'exclude': lambda f: f is None }})
    r"""If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled."""
    pipeline: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pipeline'), 'exclude': lambda f: f is None }})
    r"""Pipeline to process data before sending out to this output."""
    pq_compress: Optional[OutputGooglePubsubCompression] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqCompress'), 'exclude': lambda f: f is None }})
    r"""Codec to use to compress the persisted data."""
    pq_controls: Optional[OutputGooglePubsubPqControls] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqControls'), 'exclude': lambda f: f is None }})
    pq_max_file_size: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqMaxFileSize'), 'exclude': lambda f: f is None }})
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)."""
    pq_max_size: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqMaxSize'), 'exclude': lambda f: f is None }})
    r"""The maximum amount of disk space the queue is allowed to consume. Once reached, the system stops queueing and applies the fallback Queue-full behavior. Enter a numeral with units of KB, MB, etc."""
    pq_on_backpressure: Optional[OutputGooglePubsubQueueFullBehavior] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqOnBackpressure'), 'exclude': lambda f: f is None }})
    r"""Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_path: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqPath'), 'exclude': lambda f: f is None }})
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_strict_ordering: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pqStrictOrdering'), 'exclude': lambda f: f is None }})
    r"""Toggle this off to forward new events to receiver(s) before queue is flushed. Otherwise, default drain behavior is FIFO (first in, first out)."""
    region: Optional[OutputGooglePubsubRegion] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('region'), 'exclude': lambda f: f is None }})
    r"""Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    secret: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('secret'), 'exclude': lambda f: f is None }})
    r"""Select (or create) a stored text secret"""
    service_account_credentials: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('serviceAccountCredentials'), 'exclude': lambda f: f is None }})
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right. As an alternative, you can use environment variables (see [here](https://googleapis.dev/ruby/google-cloud-pubsub/latest/file.AUTHENTICATION.html))."""
    streamtags: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('streamtags'), 'exclude': lambda f: f is None }})
    r"""Add tags for filtering and grouping in @{product}."""
    system_fields: Optional[list[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('systemFields'), 'exclude': lambda f: f is None }})
    r"""Set of fields to automatically add to events using this output. E.g.: cribl_pipe, c*. Wildcards supported."""
    

